{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import nlp\n",
    "\n",
    "train_checkworthy = pd.read_csv(\"data/CT24_checkworthy_english/CT24_checkworthy_english_train.tsv\", sep='\\t')\n",
    "#print(train_checkworthy)\n",
    "\n",
    "with open(\"data/English_train.json\") as json_file:\n",
    "    json_strs = json_file.readlines()\n",
    "\n",
    "json_datas = [json.loads(json_str) for json_str in json_strs]\n",
    "\n",
    "train_stance = pd.DataFrame.from_dict(json_datas)\n",
    "#print(train_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json_datas[0]\n",
    "test1 = json_datas[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cw-train\n",
      "Using custom data configuration cw-dev\n",
      "Using custom data configuration cw-dev-test\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"cw-train\": nlp.load_dataset(\"csv\", name=\"cw-train\", data_files=\"data/checkworthy/CT24_checkworthy_english_train.tsv\", delimiter=\"\\t\"),\n",
    "    \"cw-dev\": nlp.load_dataset(\"csv\", name=\"cw-dev\", data_files=\"data/checkworthy/CT24_checkworthy_english_dev.tsv\", delimiter=\"\\t\"),\n",
    "    \"cw-dev-test\": nlp.load_dataset(\"csv\", name=\"cw-dev-test\", data_files=\"data/checkworthy/CT24_checkworthy_english_dev-test.tsv\", delimiter=\"\\t\"),\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw-train\n",
      "{'Sentence_id': 19099, 'Text': \"Now, let's balance the budget and protect Medicare, Medicaid, education and the environment.\", 'class_label': 'No'}\n",
      "\n",
      "cw-dev\n",
      "{'Sentence_id': 80, 'Text': \"We're consuming 50 percent of the world's cocaine.\", 'class_label': 'Yes'}\n",
      "\n",
      "cw-dev-test\n",
      "{'Sentence_id': 37440, 'Text': \"There's no way they would give it up.\", 'class_label': 'No'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name, dataset in datasets.items():\n",
    "    print(task_name)\n",
    "    print(datasets[task_name][\"train\"][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/github/zphang/zphang.github.io/blob/master/files/notebooks/Multi_task_Training_with_Transformers_NLP.ipynb#scrollTo=aVX5hFlzmLka\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "class MultiTask(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_conf_dict):\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name,\n",
    "                config=model_conf_dict[task_name]\n",
    "            )\n",
    "\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_encoder_attr_name(cls, model):\n",
    "        model_class_name = model.__class__.__name__\n",
    "        if model_class_name.startswith(\"Bert\"):\n",
    "            return \"bert\"\n",
    "        elif model_class_name.startswith(\"Roberta\"):\n",
    "            return \"roberta\"\n",
    "\n",
    "        else:\n",
    "            raise KeyError(f\"Add support for new model {model_class_name}\")\n",
    "        \n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodel_dict[task_name](**kwargs)\n",
    "#test = MultiTask(transformers.BertModel.from_pretrained(\"bert-base-uncased\"), datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "\n",
    "model_type_dict={\n",
    "        \"cw-train\": transformers.AutoModelForSequenceClassification,\n",
    "        \"cw-dev\": transformers.AutoModelForSequenceClassification,\n",
    "\n",
    "    }\n",
    "model_config_dict={\n",
    "        \"cw-train\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
    "        \"cw-dev\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "\n",
    "    }\n",
    "\n",
    "multitask_model = MultiTask.create(\n",
    "    model_name,\n",
    "    model_type_dict,\n",
    "    model_config_dict\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139656049266752\n",
      "139656049266752\n",
      "139656049266752\n"
     ]
    }
   ],
   "source": [
    "if model_name.startswith(\"roberta\"):\n",
    "    print(multitask_model.encoder.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"cw-train\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"cw-dev\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "else:\n",
    "    print(\"Exercise for the reader: add a check for other model architectures =)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
