{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlp\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cw-train\n",
      "Using custom data configuration cw-dev\n",
      "Using custom data configuration cw-train\n",
      "Using custom data configuration cw-dev\n"
     ]
    }
   ],
   "source": [
    "# train: training set for production\n",
    "# dev: significantly smaller dataset for rapid training\n",
    "# test: testset outside the training set used to measure model accuracy\n",
    "\n",
    "datasets = {\n",
    "    \"cw-train\": nlp.load_dataset(\"csv\", name=\"cw-train\", data_files=\"data/checkworthy/english_train.tsv\", delimiter=\"\\t\"),\n",
    "    \"cw-dev\": nlp.load_dataset(\"csv\", name=\"cw-dev\", data_files=\"data/checkworthy/english_dev.tsv\", delimiter=\"\\t\"),\n",
    "    \n",
    "    \"st-train\": nlp.load_dataset(\"csv\", name=\"cw-train\", data_files=\"data/stance/cleaned_train.tsv\", delimiter=\"\\t\"),\n",
    "    \"st-dev\": nlp.load_dataset(\"csv\", name=\"cw-dev\", data_files=\"data/stance/cleaned_dev.tsv\", delimiter=\"\\t\")\n",
    "}\n",
    "    #\"cw-dev-test\": nlp.load_dataset(\"csv\", name=\"cw-dev-test\", data_files=\"data/checkworthy/CT24_checkworthy_english_dev-test.tsv\", delimiter=\"\\t\"),\n",
    "\n",
    "    #TODO: Add the other datasets including the other one about stance detection\n",
    "    #TODO: Figure out which datasets is which, by that, I mean which one is training, which one is testing, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw-train\n",
      "{'Sentence_id': 19099, 'Text': \"Now, let's balance the budget and protect Medicare, Medicaid, education and the environment.\", 'class_label': 'No'}\n",
      "\n",
      "cw-dev\n",
      "{'Sentence_id': 80, 'Text': \"We're consuming 50 percent of the world's cocaine.\", 'class_label': 'Yes'}\n",
      "\n",
      "st-train\n",
      "{'': 1, 'id': 'AuRED_144', 'rumor': 'In the video.. The spread of unidentified gunmen east of the capital, Baghdad https://t.co/L18KV8tKuZ', 'label': 'REFUTES'}\n",
      "\n",
      "st-dev\n",
      "{'': 1, 'id': 'AuRED_037', 'rumor': 'Macron to Sky News: After my visit to Mrs. Fairouz last night and the visit to the Jaj Cedar Reserve, I realized the love of a large section of the Lebanese people for their President of the Republic. Fairouz also told me about her appreciation and love for the President and the reform project that the President of the Republic wants to implement. I also salute the President of the Republic and his efforts and patience. https://t.co/7pMab8yWCD', 'label': 'REFUTES'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name, dataset in datasets.items():\n",
    "    print(task_name)\n",
    "    print(datasets[task_name][\"train\"][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/github/zphang/zphang.github.io/blob/master/files/notebooks/Multi_task_Training_with_Transformers_NLP.ipynb#scrollTo=aVX5hFlzmLka\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "class MultiTask(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_conf_dict):\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name,\n",
    "                config=model_conf_dict[task_name]\n",
    "            )\n",
    "\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_encoder_attr_name(cls, model):\n",
    "        model_class_name = model.__class__.__name__\n",
    "        if model_class_name.startswith(\"Bert\"):\n",
    "            return \"bert\"\n",
    "        elif model_class_name.startswith(\"Roberta\"):\n",
    "            return \"roberta\"\n",
    "\n",
    "        else:\n",
    "            raise KeyError(f\"Add support for new model {model_class_name}\")\n",
    "        \n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict[task_name](**kwargs)\n",
    "#test = MultiTask(transformers.BertModel.from_pretrained(\"bert-base-uncased\"), datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "#from transformers import BertTokenizer, BertForSequenceClassification\n",
    "#\"cw-train\": transformers.\n",
    "model_type_dict={\n",
    "        \"cw-train\": transformers.AutoModelForSequenceClassification,\n",
    "        \"cw-dev\": transformers.AutoModelForSequenceClassification,\n",
    "    \n",
    "        \"st-train\": transformers.AutoModelForSequenceClassification,\n",
    "        \"st-dev\": transformers.AutoModelForSequenceClassification\n",
    "    }\n",
    "        #\"test\": transformers.AutoModelForTokenClassification\n",
    "\n",
    "model_config_dict={\n",
    "        \"cw-train\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3),\n",
    "        \"cw-dev\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3),\n",
    "    \n",
    "        \"st-train\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3),\n",
    "        \"st-dev\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3)\n",
    "    }\n",
    "        #\"test\": transformers.AutoConfig.from_pretrained(model_name, num_labels=5)\n",
    "#TODO: Add the other datasets in these two dictionaries. Should prolly use the AutoModelForTokenClassification for the other datasets\n",
    "multitask_model = MultiTask.create(\n",
    "    model_name,\n",
    "    model_type_dict,\n",
    "    model_config_dict\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139715869921344\n",
      "139715869921344\n",
      "139715869921344\n",
      "139715869921344\n",
      "139715869921344\n"
     ]
    }
   ],
   "source": [
    "if model_name.startswith(\"roberta\"):\n",
    "    print(multitask_model.encoder.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"cw-train\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"cw-dev\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"st-train\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"st-dev\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "else:\n",
    "    print(\"Exercise for the reader: add a check for other model architectures =)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "max_length = 128\n",
    "\n",
    "def convert_cw_train_features(batch):\n",
    "    inputs = batch['Text']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"class_label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "def convert_cw_dev_features(batch):\n",
    "    inputs = batch['Text']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"class_label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "def convert_st_train_features(batch):\n",
    "    inputs = batch['Rumor']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "def convert_st_dev_features(batch):\n",
    "    inputs = batch['Rumor']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "#TODO: Create funcs for the other datasets as well, do prolly have to do some finicky shit with TokenClassifications contra SequenceClassification\n",
    "\n",
    "convert_func_dict = {\n",
    "    \"cw-train\": convert_cw_train_features,\n",
    "    \"cw-dev\": convert_cw_dev_features,\n",
    "    \n",
    "    \"st-train\": convert_st_train_features,\n",
    "    \"st-dev\": convert_st_dev_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abje/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'dill._dill' has no attribute 'PY3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m features_dict[task_name] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phase, phase_dataset \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 16\u001b[0m     features_dict[task_name][phase] \u001b[38;5;241m=\u001b[39m \u001b[43mphase_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_func_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(task_name, phase, \u001b[38;5;28mlen\u001b[39m(phase_dataset), \u001b[38;5;28mlen\u001b[39m(features_dict[task_name][phase]))\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#Perhaps something needs to be done here?\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#pytorch iterate dataloader TypeError: new(): invalid data type 'str'\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#https://stackoverflow.com/questions/72003569/pytorch-typeerror-new-invalid-data-type-str-when-converting-nested-list\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#https://github.com/huggingface/datasets/issues/469\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#https://stackoverflow.com/questions/77241228/typeerror-invalid-data-type-str-when-using-dataloader-to-train-scibert\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/nlp/arrow_dataset.py:902\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, verbose)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# we create a unique hash from the function, current dataset file and the mapping args\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    891\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m: with_indices,\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched\u001b[39m\u001b[38;5;124m\"\u001b[39m: batched,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_nullable\u001b[39m\u001b[38;5;124m\"\u001b[39m: disable_nullable,\n\u001b[1;32m    901\u001b[0m     }\n\u001b[0;32m--> 902\u001b[0m     cache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cache_file_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cache_file_name) \u001b[38;5;129;01mand\u001b[39;00m load_from_cache_file:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/nlp/arrow_dataset.py:756\u001b[0m, in \u001b[0;36mDataset._get_cache_file_path\u001b[0;34m(self, function, cache_kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m previous_files_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(k) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_files\n\u001b[1;32m    754\u001b[0m )\n\u001b[1;32m    755\u001b[0m cache_kwargs_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(k) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cache_kwargs\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m--> 756\u001b[0m function_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m output_hash \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39mmd5(\n\u001b[1;32m    758\u001b[0m     previous_files_string\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m cache_kwargs_string\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m function_bytes\n\u001b[1;32m    759\u001b[0m )\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    760\u001b[0m cache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m output_hash \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/nlp/utils/py_utils.py:278\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"pickle an object to a string\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m file \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m--> 278\u001b[0m \u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/nlp/utils/py_utils.py:271\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file):\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"pickle an object to a file\"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     \u001b[43mPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/dill/_dill.py:420\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     logger\u001b[38;5;241m.\u001b[39mtrace_setup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 420\u001b[0m     \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/pickle.py:481\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/dill/_dill.py:414\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    412\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: attribute lookup builtins.generator failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m GeneratorType\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 414\u001b[0m \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_persistent_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/pickle.py:554\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    552\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/dill/_dill.py:1985\u001b[0m, in \u001b[0;36msave_function\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict:\n\u001b[1;32m   1983\u001b[0m     state \u001b[38;5;241m=\u001b[39m state, state_dict\n\u001b[0;32m-> 1985\u001b[0m \u001b[43m_save_with_postproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpickler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_create_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__defaults__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclosure\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostproc_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostproc_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# Lift closure cell update to earliest function (#458)\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _postproc:\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/dill/_dill.py:1098\u001b[0m, in \u001b[0;36m_save_with_postproc\u001b[0;34m(pickler, reduction, is_pickler_dill, obj, postproc_list)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     pickler\u001b[38;5;241m.\u001b[39m_postproc[\u001b[38;5;28mid\u001b[39m(obj)] \u001b[38;5;241m=\u001b[39m postproc_list\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO: Use state_setter in Python 3.8 to allow for faster cPickle implementations\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_pickler_dill:\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# pickler.x -= 1\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;66;03m# print(pickler.x*' ', 'pop', obj, id(obj))\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m     postproc \u001b[38;5;241m=\u001b[39m pickler\u001b[38;5;241m.\u001b[39m_postproc\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mid\u001b[39m(obj))\n",
      "File \u001b[0;32m/usr/lib/python3.12/pickle.py:686\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     save(func)\n\u001b[0;32m--> 686\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m     write(REDUCE)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/dill/_dill.py:414\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    412\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: attribute lookup builtins.generator failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m GeneratorType\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 414\u001b[0m \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_persistent_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/pickle.py:554\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    552\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/pickle.py:896\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    894\u001b[0m write(MARK)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[0;32m--> 896\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(obj) \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;66;03m# Subtle.  d was not in memo when we entered save_tuple(), so\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# the process of saving the tuple's elements must have saved\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# could have been done in the \"for element\" loop instead, but\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# recursive tuples are a rare thing.\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     get \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(memo[\u001b[38;5;28mid\u001b[39m(obj)][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/dill/_dill.py:414\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    412\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: attribute lookup builtins.generator failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m GeneratorType\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 414\u001b[0m \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_persistent_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/pickle.py:554\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    552\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/uis/datautvinning/DAT550_mtl_facts/env/lib/python3.12/site-packages/nlp/utils/py_utils.py:304\u001b[0m, in \u001b[0;36msave_code\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m    302\u001b[0m co_firstlineno \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mco_filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mco_firstlineno\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# The rest is the same as in the original dill implementation\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPY3\u001b[49m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mco_posonlyargcount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    306\u001b[0m         args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    307\u001b[0m             obj\u001b[38;5;241m.\u001b[39mco_argcount,\n\u001b[1;32m    308\u001b[0m             obj\u001b[38;5;241m.\u001b[39mco_posonlyargcount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m             obj\u001b[38;5;241m.\u001b[39mco_cellvars,\n\u001b[1;32m    323\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dill._dill' has no attribute 'PY3'"
     ]
    }
   ],
   "source": [
    "columns_dict = {\n",
    "    \"cw-train\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \"cw-dev\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \n",
    "    \"st-train\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \"st-dev\": ['input_ids', 'attention_mask', 'labels'],\n",
    "}   \n",
    "    #\"cw-dev\": ['Sentence_id', 'Text', 'class_label'],\n",
    "    #\"test\": ['id', 'rumor', 'label', 'timeline', 'evidence']\n",
    "#TODO: Add shit here as well, above, not below, or perhaps below as well, who knows. \n",
    "\n",
    "features_dict = {}\n",
    "for task_name, dataset in datasets.items():\n",
    "    features_dict[task_name] = {}\n",
    "    for phase, phase_dataset in dataset.items():\n",
    "        features_dict[task_name][phase] = phase_dataset.map(\n",
    "            convert_func_dict[task_name],\n",
    "            batched=True,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))\n",
    "        #Perhaps something needs to be done here?\n",
    "        #pytorch iterate dataloader TypeError: new(): invalid data type 'str'\n",
    "        #https://stackoverflow.com/questions/72003569/pytorch-typeerror-new-invalid-data-type-str-when-converting-nested-list\n",
    "        #https://github.com/huggingface/datasets/issues/469\n",
    "        #https://stackoverflow.com/questions/77241228/typeerror-invalid-data-type-str-when-using-dataloader-to-train-scibert\n",
    "        features_dict[task_name][phase].set_format(\n",
    "            type='torch', \n",
    "            columns=columns_dict[task_name],\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "class NLPDataCollator(DefaultDataCollator):\n",
    "    def collate_batch(self, features):\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "                else:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "                batch = {\"labels\": labels}\n",
    "            for k ,v in first.items():\n",
    "                if k != \"labels\":\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "            return transformers.DefaultDataCollator().collate_batch(features)\n",
    "    \n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    def to(self, device):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderWithTaskname:\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "    #https://stackoverflow.com/questions/77241228/typeerror-invalid-data-type-str-when-using-dataloader-to-train-scibert\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset)\n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "    \n",
    "    def __iter__(self):\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#from transformers.data.data_collator import torch_default_data_collator\n",
    "from transformers.trainer_utils import is_tf_available\n",
    "from transformers.trainer import get_dataloader_sampler\n",
    "#from accelerate.state import is_tf_available\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "class MultitaskTrainer(Trainer):\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        \n",
    "        #train_sampler = (\n",
    "        #    RandomSampler(train_dataset)\n",
    "        #    if self.args.local_rank == -1\n",
    "        #    else DistributedSampler(train_dataset) # This cucks the code up... something about missing default proc group. Prolly sumtin to do with concurrency.\n",
    "        #)\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.args.train_batch_size,\n",
    "                sampler=train_sampler,\n",
    "                collate_fn=self.data_collator.collate_batch,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        #if is_tf_available():\n",
    "        #    data_loader = nn.pl.ParallelLoader(data_loader, [self.args.device]).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        loader = MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, dataset)\n",
    "            for task_name, dataset in self.train_dataset.items()\n",
    "        })\n",
    "        \n",
    "        return loader\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    task_name: dataset[\"train\"]\n",
    "    for task_name, dataset in features_dict.items()\n",
    "}\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"models/multitask_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=8,\n",
    "        save_steps=5000,\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT550",
   "language": "python",
   "name": "dat550"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
