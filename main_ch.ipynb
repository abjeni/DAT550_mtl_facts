{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlp\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stance = nlp.Features(\n",
    "    {\n",
    "        \"id\": nlp.Value(\"string\"),\n",
    "        \"rumor\": nlp.Value(\"string\"),\n",
    "        \"label\": nlp.Value(\"string\"),\n",
    "        \"timeline\": nlp.Sequence(nlp.Value(\"string\")),\n",
    "        \"evidence\": nlp.Sequence(nlp.Value(\"string\")),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: training set for production\n",
    "# dev: significantly smaller dataset for rapid training\n",
    "# test: testset outside the training set used to measure model accuracy\n",
    "\n",
    "datasets = {\n",
    "    \"cw-train\": nlp.load_dataset(\"csv\", name=\"cw-train\", data_files=\"data/checkworthy/english_train.tsv\", delimiter=\"\\t\"),\n",
    "    \"cw-dev\": nlp.load_dataset(\"csv\", name=\"cw-dev\", data_files=\"data/checkworthy/english_dev.tsv\", delimiter=\"\\t\"),\n",
    "    \"st-train\": nlp.load_dataset(\"json\", name=\"st-train\", data_files=\"data/stance/cleaned_train_extra.json\"),\n",
    "    \"st-dev\": nlp.load_dataset(\"json\", name=\"st-dev\", data_files=\"data/stance/cleaned_dev_extra.json\")\n",
    "}\n",
    "    #TODO: Add the other datasets including the other one about stance detection\n",
    "    #TODO: Figure out which datasets is which, by that, I mean which one is training, which one is testing, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pds = pd.read_json(\"data/stance/cleaned_train.json\", lines=True)\n",
    "#pds[\"timeline\"] = pds[\"timeline\"].apply(lambda timeline: [x[2] for x in timeline])\n",
    "#pds[\"evidence\"] = pds[\"evidence\"].apply(lambda evidence: [x[2] for x in evidence])\n",
    "#pds[\"timeline\"] = pds[\"timeline\"][0]\n",
    "#pds[\"evidence\"] = pds[\"evidence\"][0]\n",
    "#d = [{\"id\": } for i in pds.to_dict]\n",
    "#tasts = pds.to_dict(orient=\"records\")\n",
    "#tasta = json.dumps(tasts)\n",
    "#with open(\"data/stance/cleaned_train_extra.json\", \"w\") as f:\n",
    "#    for element in tasts:\n",
    "#        json.dump(element, f)\n",
    "#        f.write(\"\\n\")\n",
    "#pds.to_json(\"data/stance/cleaned_train_extra.json\", orient=\"index\")\n",
    "#tast = Dataset.from_pandas(pds)\n",
    "#test = pds.iloc[:1][\"timeline\"][0]\n",
    "\n",
    "#tests = datasets[\"cw-train\"]\n",
    "def clean_to_extra_clean(f_loc, fname, end_fname):\n",
    "    df = pd.read_json(os.path.join(f_loc, fname), lines=True)\n",
    "    df[\"timeline\"] = df[\"timeline\"].apply(lambda timeline: [x[2] for x in timeline])\n",
    "    df[\"evidence\"] = df[\"evidence\"].apply(lambda evidence: [x[2] for x in evidence])\n",
    "    json_format = df.to_dict(orient=\"records\")\n",
    "    with open(os.path.join(f_loc, end_fname), \"w\") as f:\n",
    "        for element in json_format:\n",
    "            json.dump(element, f)\n",
    "            f.write(\"\\n\")\n",
    "    return\n",
    "#The following function generates files, these are already created, no need to do it again\n",
    "#clean_to_extra_clean(\"data/stance\", \"cleaned_train.json\", \"cleaned_train_extra.json\")\n",
    "#clean_to_extra_clean(\"data/stance\", \"cleaned_dev.json\", \"cleaned_dev_extra.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name, dataset in datasets.items():\n",
    "    print(task_name)\n",
    "    print(datasets[task_name][\"train\"][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/github/zphang/zphang.github.io/blob/master/files/notebooks/Multi_task_Training_with_Transformers_NLP.ipynb#scrollTo=aVX5hFlzmLka\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "class MultiTask(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_conf_dict):\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name,\n",
    "                config=model_conf_dict[task_name]\n",
    "            )\n",
    "\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_encoder_attr_name(cls, model):\n",
    "        model_class_name = model.__class__.__name__\n",
    "        if model_class_name.startswith(\"Bert\"):\n",
    "            return \"bert\"\n",
    "        elif model_class_name.startswith(\"Roberta\"):\n",
    "            return \"roberta\"\n",
    "\n",
    "        else:\n",
    "            raise KeyError(f\"Add support for new model {model_class_name}\")\n",
    "        \n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict[task_name](**kwargs)\n",
    "#test = MultiTask(transformers.BertModel.from_pretrained(\"bert-base-uncased\"), datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "#from transformers import BertTokenizer, BertForSequenceClassification\n",
    "#\"cw-train\": transformers.\n",
    "model_type_dict={\n",
    "        \"cw-train\": transformers.AutoModelForSequenceClassification,\n",
    "        \"cw-dev\": transformers.AutoModelForSequenceClassification,\n",
    "    \n",
    "        \"st-train\": transformers.AutoModelForSequenceClassification,\n",
    "        \"st-dev\": transformers.AutoModelForSequenceClassification\n",
    "    }\n",
    "        #\"test\": transformers.AutoModelForTokenClassification\n",
    "\n",
    "model_config_dict={\n",
    "        \"cw-train\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3),\n",
    "        \"cw-dev\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3),\n",
    "    \n",
    "        \"st-train\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3),\n",
    "        \"st-dev\": transformers.AutoConfig.from_pretrained(model_name, num_labels=3)\n",
    "    }\n",
    "        #\"test\": transformers.AutoConfig.from_pretrained(model_name, num_labels=5)\n",
    "#TODO: Add the other datasets in these two dictionaries. Should prolly use the AutoModelForTokenClassification for the other datasets\n",
    "multitask_model = MultiTask.create(\n",
    "    model_name,\n",
    "    model_type_dict,\n",
    "    model_config_dict\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name.startswith(\"roberta\"):\n",
    "    print(multitask_model.encoder.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"cw-train\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"cw-dev\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"st-train\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"st-dev\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "else:\n",
    "    print(\"Exercise for the reader: add a check for other model architectures =)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "max_length = 128\n",
    "\n",
    "def convert_cw_train_features(batch):\n",
    "    inputs = batch['Text']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"class_label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "def convert_cw_dev_features(batch):\n",
    "    inputs = batch['Text']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"class_label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "def convert_st_train_features(batch):\n",
    "    inputs = batch['Rumor']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "def convert_st_dev_features(batch):\n",
    "    inputs = batch['Rumor']\n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(batch[\"label\"])\n",
    "    features[\"labels\"] = targets\n",
    "    #features[\"labels\"] = batch[\"class_label\"]\n",
    "    return features\n",
    "\n",
    "#TODO: Create funcs for the other datasets as well, do prolly have to do some finicky shit with TokenClassifications contra SequenceClassification\n",
    "\n",
    "convert_func_dict = {\n",
    "    \"cw-train\": convert_cw_train_features,\n",
    "    \"cw-dev\": convert_cw_dev_features,\n",
    "    \n",
    "    \"st-train\": convert_st_train_features,\n",
    "    \"st-dev\": convert_st_dev_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dict = {\n",
    "    \"cw-train\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \"cw-dev\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \n",
    "    \"st-train\": ['input_ids', 'attention_mask', 'labels'],\n",
    "    \"st-dev\": ['input_ids', 'attention_mask', 'labels'],\n",
    "}   \n",
    "    #\"cw-dev\": ['Sentence_id', 'Text', 'class_label'],\n",
    "    #\"test\": ['id', 'rumor', 'label', 'timeline', 'evidence']\n",
    "#TODO: Add shit here as well, above, not below, or perhaps below as well, who knows. \n",
    "\n",
    "features_dict = {}\n",
    "for task_name, dataset in datasets.items():\n",
    "    features_dict[task_name] = {}\n",
    "    for phase, phase_dataset in dataset.items():\n",
    "        features_dict[task_name][phase] = phase_dataset.map(\n",
    "            convert_func_dict[task_name],\n",
    "            batched=True,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))\n",
    "        #Perhaps something needs to be done here?\n",
    "        #pytorch iterate dataloader TypeError: new(): invalid data type 'str'\n",
    "        #https://stackoverflow.com/questions/72003569/pytorch-typeerror-new-invalid-data-type-str-when-converting-nested-list\n",
    "        #https://github.com/huggingface/datasets/issues/469\n",
    "        #https://stackoverflow.com/questions/77241228/typeerror-invalid-data-type-str-when-using-dataloader-to-train-scibert\n",
    "        features_dict[task_name][phase].set_format(\n",
    "            type='torch', \n",
    "            columns=columns_dict[task_name],\n",
    "        )\n",
    "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "class NLPDataCollator(DefaultDataCollator):\n",
    "    def collate_batch(self, features):\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "                else:\n",
    "                    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "                batch = {\"labels\": labels}\n",
    "            for k ,v in first.items():\n",
    "                if k != \"labels\":\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "            return transformers.DefaultDataCollator().collate_batch(features)\n",
    "    \n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    def to(self, device):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderWithTaskname:\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "    #https://stackoverflow.com/questions/77241228/typeerror-invalid-data-type-str-when-using-dataloader-to-train-scibert\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset)\n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "    \n",
    "    def __iter__(self):\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#from transformers.data.data_collator import torch_default_data_collator\n",
    "from transformers.trainer_utils import is_tf_available\n",
    "from transformers.trainer import get_dataloader_sampler\n",
    "#from accelerate.state import is_tf_available\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "class MultitaskTrainer(Trainer):\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        \n",
    "        #train_sampler = (\n",
    "        #    RandomSampler(train_dataset)\n",
    "        #    if self.args.local_rank == -1\n",
    "        #    else DistributedSampler(train_dataset) # This cucks the code up... something about missing default proc group. Prolly sumtin to do with concurrency.\n",
    "        #)\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.args.train_batch_size,\n",
    "                sampler=train_sampler,\n",
    "                collate_fn=self.data_collator.collate_batch,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        #if is_tf_available():\n",
    "        #    data_loader = nn.pl.ParallelLoader(data_loader, [self.args.device]).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        loader = MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, dataset)\n",
    "            for task_name, dataset in self.train_dataset.items()\n",
    "        })\n",
    "        \n",
    "        return loader\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    task_name: dataset[\"train\"]\n",
    "    for task_name, dataset in features_dict.items()\n",
    "}\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"models/multitask_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=8,\n",
    "        save_steps=5000,\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
